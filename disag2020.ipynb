{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa95de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting maps\n",
    "import pandas as pd # standard python data library\n",
    "import geopandas as gp # the geo-version of pandas\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)   \n",
    "\n",
    "wd = os.getcwd()\n",
    "pdir = os.path.dirname(wd)\n",
    "f2020 = os.path.join(pdir,'2020')\n",
    "fcsv = os.path.join(f2020,'csv')\n",
    "bg_csv = os.path.join(fcsv,'bg_cvap_2020_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d576e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counties(state_fips):\n",
    "    \"\"\"Inputs: state fips code\n",
    "    Process: Retrieves a list of counties in the given state from the Census API.  \n",
    "    Outputs: A pandas dataframe of county fips codes in the state. \"\"\"\n",
    "    #uses the fips input into the census api\n",
    "    resp = requests.get(\n",
    "        \"https://api.census.gov/data/2020/dec/pl\"\n",
    "        \"?get=NAME&for=county:*&in=state:{}\".format(state_fips)  #uses the fips input to locate the state\n",
    "    )\n",
    "    #retrieves the data as a json \n",
    "    header, *rows = resp.json()\n",
    "    #county column is \"county\"\n",
    "    county_column_index = header.index(\"county\")\n",
    "    county_fips = set(row[county_column_index] for row in rows) #sequence of counties \n",
    "    county_name_index = header.index(\"NAME\")\n",
    "    county_names = set(row[county_name_index] for row in rows)\n",
    "    county_fips = np.array(list(county_fips))\n",
    "    county_names = np.array(list(county_names))\n",
    "    df = pd.DataFrame({'COUNTYFP20': county_fips, 'COUNTYNAMES': county_names}) #make pd dataframe of arrays\n",
    "    df['COUNTY_STATE_FIPS']=state_fips + df['COUNTYFP20']\n",
    "    return df #returns the fips codes of all counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2020pl_data(fip, geog, y, CENSUS_API_KEY = 'ef8d7d2d71226a4e4f86b6ee741c8d8f979d6c7b'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    Outputs:\n",
    "    \"\"\"\n",
    "    HOST = \"https://api.census.gov/data\"\n",
    "    year = \"2020\"\n",
    "    dataset = \"dec/pl\"\n",
    "    base_url = \"/\".join([HOST, year, dataset])\n",
    "    # The variables we want are NAME and total population\n",
    "    #get_variables = [\"NAME\", \"P001001\"]   \n",
    "    data = []\n",
    "    variables = ['P4_001N','P4_003N','P4_007N','P4_008N','P4_006N','P4_009N','P4_005N','P4_014N','P4_015N','P4_013N','P4_018N','P4_011N','P4_002N','P2_001N','P2_003N','P2_007N','P2_008N','P2_006N','P2_009N','P2_005N','P2_014N','P2_015N','P2_013N','P2_018N','P2_011N','P2_002N']\n",
    "    new_names = ['CVAP_TOT'+y,'CVAP_NHS'+y,'CVAP_AIA'+y,'CVAP_ASN'+y,'CVAP_BLK'+y,'CVAP_NHP'+y,'CVAP_WHT'+y,'CVAP_AIW'+y,'CVAP_ASW'+y,'CVAP_BLW'+y,'CVAP_AIB'+y,'CVAP_2OM'+y,'CVAP_HSP'+y,'C_TOT'+y,'C_NHS'+y,'C_AIA'+y,'C_ASN'+y,'C_BLK'+y,'C_NHP'+y,'C_WHT'+y,'C_AIW'+y,'C_ASW'+y,'C_BLW'+y,'C_AIB'+y,'C_2OM'+y,'C_HSP'+y]\n",
    "    #print(len(variables),len(new_names))\n",
    "    rename_dict = dict(zip(variables,new_names))\n",
    "    #for k,v in rename_dict.items():\n",
    "        #print(k, v)\n",
    "    counties_codes_TEST = counties(fip)\n",
    "    counties_fips = counties_codes_TEST['COUNTY_STATE_FIPS']\n",
    "    print('starting to collect data for ' + geog + ' ' + fip)\n",
    "    for county in counties_fips:\n",
    "       # for county_code in state_county_dict[fip]:\n",
    "        predicates = {}\n",
    "        predicates[\"get\"] = \",\".join(variables)\n",
    "        if geog == 'b':\n",
    "            predicates[\"for\"] = \"block:*\"\n",
    "        if geog == 'bg':\n",
    "            predicates[\"for\"] = \"block group:*\"\n",
    "        predicates[\"in\"] = \"state:\" + fip + \"+county:\" + county[2:]\n",
    "        predicates[\"key\"] = CENSUS_API_KEY\n",
    "        # Write the result to a response object:\n",
    "        response = requests.get(base_url, params=predicates)\n",
    "        #print(response.url)\n",
    "        #print(response)\n",
    "        col_names = response.json()[0]        \n",
    "        data = data + response.json()[1:]\n",
    "        print(\"found data for: \" + county + \"!\")\n",
    "    print('done collecting data for', fip)\n",
    "    geoids = []  # initialize geoid vector\n",
    "    pop_data = pd.DataFrame(columns=col_names, data=data)\n",
    "    pop_data.rename(columns=rename_dict, inplace=True)\n",
    "    cols = [i for i in pop_data.columns if i not in [\"NAME\",\"state\",\"county\",\"tract\",'block group',\"block\"]]\n",
    "    for col in cols:\n",
    "        pop_data[col]=pd.to_numeric(pop_data[col])\n",
    "    for index, row in pop_data.iterrows():\n",
    "        # make changes here for tracts\n",
    "        if geog == 'b':\n",
    "            geoid = row[\"state\"] + row[\"county\"] + row[\"tract\"] + row[\"block\"]\n",
    "        if geog == 'bg':\n",
    "            geoid = row[\"state\"] + row[\"county\"] + row[\"tract\"] + row[\"block group\"]\n",
    "        geoids.append(geoid)\n",
    "        \n",
    "        \n",
    "    if geog == 'b':\n",
    "        pop_data[\"block group\"] = pop_data[\"block\"].apply(lambda x: x[0])\n",
    "        pop_data[\"BG_GEOID\"] = pop_data.apply(lambda x: x[\"state\"] + x[\"county\"] + x[\"tract\"] + x[\"block group\"], axis=1)\n",
    "    pop_data[\"GEOID\"] = geoids\n",
    "    \n",
    "    pivot = pop_data\n",
    "\n",
    "    pivot['C_AIA'+y] = pivot.apply(lambda x: x['C_AIA'+y]+x['C_AIB'+y]+x['C_AIW'+y],axis=1)\n",
    "    pivot['CVAP_AIA'+y] = pivot.apply(lambda x: x['CVAP_AIA'+y]+x['CVAP_AIB'+y]+x['CVAP_AIW'+y],axis=1)\n",
    "    pivot['C_BLK'+y] = pivot.apply(lambda x: x['C_BLK'+y]+x['C_BLW'+y]+x['C_AIB'+y],axis=1)    \n",
    "    pivot['CVAP_BLK'+y] = pivot.apply(lambda x: x['CVAP_BLK'+y]+x['CVAP_BLW'+y]+x['CVAP_AIB'+y],axis=1)\n",
    "    pivot['C_ASN'+y] = pivot.apply(lambda x: x['C_ASN'+y] + x['C_ASW'+y],axis=1)\n",
    "    pivot['CVAP_ASN'+y] = pivot.apply(lambda x: x['CVAP_ASN'+y] + x['CVAP_ASW'+y],axis=1)\n",
    "    pivot['CVAP_2OM'+y] = pivot.apply(lambda x: x['CVAP_2OM'+y] - x['CVAP_AIB'+y] - x['CVAP_AIW'+y] - x['CVAP_BLW'+y] - x['CVAP_ASW'+y],axis=1)\n",
    "    pivot['C_2OM'+y] = pivot.apply(lambda x: x['C_2OM'+y] - x['C_AIB'+y] - x['C_AIW'+y] - x['C_BLW'+y] - x['C_ASW'+y],axis=1)    \n",
    "    \n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f044ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bg_row(bg_geoid, col, bg_df, iteration = 0, disp = False):\n",
    "    bg_df['GEOID'] = bg_df['GEOID'].astype(str)\n",
    "    bg_row = bg_df[bg_df['GEOID']==bg_geoid].copy()\n",
    "    #display(bg_row)\n",
    "    if disp == True:\n",
    "        print('GEOID is', bg_geoid)\n",
    "        print('Length of DF: ', len(bg_row))\n",
    "        display(bg_row)\n",
    "    val = list(bg_row[col])\n",
    "\n",
    "    try:\n",
    "        val = val[0]\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        print('BG GEOID: ', bg_geoid)\n",
    "    return int(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ab(fips):\n",
    "    values = ['01','02','04','05','06','08','09','10','12','13','15','16','17','18','19','20','21','22','23',\n",
    "                  '24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','44','45','46',\n",
    "                  '47','48','49','50','51','53','54','55','56']\n",
    "    keys = ['al','ak','az','ar','ca','co','ct','de','fl','ga','hi','id','il','in','ia','ks','ky','la','me','md','ma','mi','mn','ms','mo','mt','ne','nv','nh','nj','nm','ny','nc','nd','oh','ok','or','pa','ri','sc','sd','tn','tx','ut','vt','va','wa','wv','wi','wy']\n",
    "    dictionary = dict(zip(keys,values))\n",
    "    state_ab = ''\n",
    "    for key, value in dictionary.items(): \n",
    "        if value == fips: \n",
    "            state_ab=key\n",
    "    return state_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4769c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_mismatch(b_df,cvap,i,y):\n",
    "    cur_sum = b_df[i].sum()\n",
    "    cvap_sum = cvap[i].sum()\n",
    "    bg_ids = list(b_df['BG_GEOID'].unique())\n",
    "    if int(cvap_sum)!= 0:\n",
    "        rem = cvap_sum-cur_sum\n",
    "        for bg_id in bg_ids:\n",
    "            b_sub = b_df[b_df['BG_GEOID']==bg_id].copy()\n",
    "            cvap_sub = cvap[cvap['GEOID']==bg_id].copy()\n",
    "            b_sub_tot_i = b_sub[i].sum()\n",
    "            cvap_sub_tot_i = cvap_sub[i].sum()\n",
    "            bg_rem = cvap_sub_tot_i-b_sub_tot_i\n",
    "            if int(bg_rem) != 0:\n",
    "                if 'C_' in bg_id:\n",
    "                    c_tot_sum = int(cvap_sub['C_TOT'+y].sum())\n",
    "                    if int(c_tot_sum)!= 0:\n",
    "                        if i!='C_TOT'+y:\n",
    "                            b_sub[i] = b_sub['C_TOT'+y].apply(lambda x: float(x/c_tot_sum))\n",
    "                            b_sub[i] = b_sub[i].apply(lambda x: x*bg_rem)\n",
    "                        else:\n",
    "                            val = c_tot_sum/len(b_sub)\n",
    "                            b_sub[i] = val\n",
    "                    else:\n",
    "                        b_sub[i] = float(0)\n",
    "                else:\n",
    "                    cvap_tot_sum = int(cvap_sub['CVAP_TOT'+y].sum())\n",
    "                    if int(cvap_tot_sum)!= 0:\n",
    "                        if i!='CVAP_TOT'+y:\n",
    "                            b_sub[i] = b_sub['CVAP_TOT'+y].apply(lambda x: float(x/cvap_tot_sum))\n",
    "                            b_sub[i] = b_sub[i].apply(lambda x: x*bg_rem)\n",
    "                        else:\n",
    "                            val = float(cvap_tot_sum/len(b_sub))\n",
    "                            b_sub[i] = val\n",
    "                    else:\n",
    "                        b_sub[i] = float(0)\n",
    "                b_df = b_df[b_df['BG_GEOID']!=bg_id].copy()\n",
    "                b_df = pd.concat([b_sub,b_df])\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        b_df[i] = float(0)\n",
    "    new_sum = b_df[i].sum()\n",
    "    for bg_id in bg_ids:\n",
    "        b_sub = b_df[b_df['BG_GEOID']==bg_id].copy()\n",
    "        cvap_sub = cvap[cvap['GEOID']==bg_id].copy()\n",
    "        b_sub_tot_i = b_sub[i].sum()\n",
    "        cvap_sub_tot_i = cvap_sub[i].sum()\n",
    "        if round(cvap_sub_tot_i)!= round(b_sub_tot_i):\n",
    "            print('***THERE IS STILL A MISMATCH***')\n",
    "            print('CVAP TOTAL FOR BG ', str(bg_id), ' COLUMN ', i, ' IS: ', str(cvap_sub_tot_i))\n",
    "            print('NEW TOTAL FOR BG ', str(bg_id), ' COLUMN ', i, ' IS: ', str(b_sub_tot_i))\n",
    "    if round(cvap_sum)!=round(new_sum):\n",
    "        print('SUMS ARE NOT EQUAL')\n",
    "        print('CVAP SUM: ', str(cvap_sum))\n",
    "        print('NEW SUM: ', str(new_sum))\n",
    "    return b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_types(wd = os.getcwd()):\n",
    "    ak_csv = pd.read_csv(os.path.join(os.path.join(bg_csv,'ak_cvap_2020_bg'),'ak_cvap_2020_bg.csv'))\n",
    "    cvap_cols = list(ak_csv.columns)\n",
    "    type_list = []\n",
    "    for i in cvap_cols:\n",
    "        if i.startswith('CVAP'):\n",
    "            type_list.append(\"int\")\n",
    "        if i.startswith('C_'):\n",
    "            type_list.append(\"int\")\n",
    "        else:\n",
    "            type_list.append(\"object\")\n",
    "\n",
    "    dtype_dict = dict(zip(cvap_cols,type_list))\n",
    "    return dtype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio_table(fips,year):\n",
    "    y = year[-2:]\n",
    "    dtype_dict = set_types()\n",
    "    b = get_2020pl_data(fips,'b',y)\n",
    "    bg = b.groupby('BG_GEOID').sum()\n",
    "    bg.reset_index(inplace=True)\n",
    "    bg['GEOID'] = bg['BG_GEOID'].astype(str)\n",
    "    sa = assign_ab(fips)\n",
    "    print('On state: ', sa.upper())\n",
    "    wd = os.getcwd()\n",
    "    cvap_folders  = bg_csv\n",
    "    for i in os.listdir(cvap_folders):\n",
    "        if '.zip' not in i:\n",
    "            if i.startswith(sa):\n",
    "                folder_path = os.path.join(cvap_folders,i)\n",
    "                for j in os.listdir(folder_path):\n",
    "                    if j.endswith('.csv'):\n",
    "                        csv_path = os.path.join(folder_path,j)\n",
    "    cvap = pd.read_csv(csv_path,dtype=dtype_dict)\n",
    "    if len(bg)!=len(cvap):\n",
    "        print('LENGHTS OF BG AND CVAP DO NOT EQUAL')\n",
    "        return \n",
    "    cvap['GEOID'] = cvap['GEOID'].astype(str)\n",
    "    no_data_cols = ['GEOID','BG_GEOID','state','county','tract','block', 'block group','NAME']\n",
    "    start_process_time = time.time()\n",
    "    for i in b.columns:\n",
    "        if i not in no_data_cols:\n",
    "            print('Starting column ', i, '...')\n",
    "            col_start = time.time()\n",
    "            print('STARTING RATIOS!')\n",
    "            b[i] = b.apply(lambda x: float(int((x[i]))/int(get_bg_row(x['BG_GEOID'], i, bg))) * get_bg_row(x['BG_GEOID'],i,cvap) if get_bg_row(x['BG_GEOID'], i, bg) != 0 else 0,axis=1)\n",
    "            if round(b[i].sum())!=round(cvap[i].sum()):\n",
    "                print('PRIOR TO MISMATCH PROCESSING BLOCK TOTAL IS: ', str(b[i].sum()))\n",
    "                print('PRIOR TO MISMATCH PROCESSING CVAP TOTAL IS: ', str(cvap[i].sum()))\n",
    "                b = mod_mismatch(b,cvap,i,y)\n",
    "            col_end = time.time()\n",
    "            diff_sec = col_end-col_start\n",
    "            diff_min = diff_sec/60\n",
    "            print('TIME TAKEN FOR ', i, ': ', str(diff_min), ' MINUTES.')\n",
    "        else:\n",
    "            continue\n",
    "    cols_to_drop = []\n",
    "    for i in list(b.columns):\n",
    "        if i.startswith('floor'):\n",
    "            cols_to_drop.append(i)\n",
    "        elif i.startswith('re'):\n",
    "            cols_to_drop.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    b.drop(columns=cols_to_drop,inplace=True)\n",
    "    for i in b.columns:\n",
    "        if i not in no_data_cols:\n",
    "            #print('\\nCVAP ORIGINAL TOTAL FOR ',i,':',cvap[i].sum())\n",
    "            #print('NEW ALLOCATED BLOCK TOTALS FOR',i,':',b[i].sum())\n",
    "            print('\\n')\n",
    "            if round(cvap[i].sum())==round(b[i].sum()):\n",
    "                print(i, ' IS EQUAL.')\n",
    "                continue\n",
    "            else:\n",
    "                print('NOT EQUAL FOR ',i)\n",
    "                print('CVAP ',i, ' IS: ',str(cvap[i].sum()))\n",
    "                print('NEW BLOCKS ', i , ' IS: ', str(b[i].sum()))\n",
    "    #display(b.head())\n",
    "    end_process_time = time.time()\n",
    "    process_time_sec = end_process_time - start_process_time\n",
    "    process_time_min = process_time_sec/60\n",
    "    process_time_hr = process_time_min/60\n",
    "    print(sa.upper(), ' TOTAL COLUMN PROCESSING TIME TO GET TO 2020 BLOCKS: ', str(process_time_hr), ' HOURS.')\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disag(fips,year):\n",
    "    disag = get_ratio_table(fips,year)\n",
    "    disag_folder = os.path.join(os.getcwd(),'DISAG_CSV')\n",
    "    if not os.path.exists(disag_folder):\n",
    "        os.mkdir(disag_folder)\n",
    "    sa = assign_ab(fips)\n",
    "    name = '_'.join([sa.lower(),'cvap',year,'2020','b.csv'])\n",
    "    extract_path = os.path.join(disag_folder,name)\n",
    "    disag.to_csv(extract_path,index=False)#Just added, need to re-run with this\n",
    "    return disag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f866fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "disag_list = ['01','02','04','05','08','09','10','12','13','15','16','17','18','19','20','21','22','23',\n",
    "                  '24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','44','45','46',\n",
    "                  '47','48','49','50','51','53','54','55','56','06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_disag(fips_list = disag_list,years_list = ['2020']):\n",
    "    datasets = []\n",
    "    abbrv = []\n",
    "    for y in years_list:\n",
    "        for state in disag_list:\n",
    "            sa = assign_ab(state)\n",
    "            print('STARTING: ', sa.upper(), ' ', y)\n",
    "            start_time = time.time()\n",
    "            disag_dataset = disag(state,y)\n",
    "            datasets.append(disag_dataset)\n",
    "            abbrv.append(sa)\n",
    "            print('FINISHED: ', sa.upper(), ' ', y)\n",
    "            end_time = time.time()\n",
    "            diff_sec = end_time - start_time\n",
    "            diff_min = diff_sec/60\n",
    "            diff_hr = diff_min/60\n",
    "            print('TOTAL TIME TO RUN FOR ', sa.upper(),' ', y, ': ', str(diff_hr))\n",
    "    datasets_dict = dict(zip(abbrv,datasets))\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75e56b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = run_disag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fc823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_datasets(dataset_dict = datasets):\n",
    "    for k,v in dataset_dict.items():\n",
    "        print(k, ' IS DONE.')\n",
    "view_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66d25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
